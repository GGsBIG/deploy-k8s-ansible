# Kubernetes æœå‹™å®Œæ•´å»ºç½®å®‰è£æŒ‡å—

## æ¦‚è¿°
æœ¬æ–‡æª”æä¾›å®Œæ•´çš„ Kubernetes æœå‹™å»ºç½®å®‰è£æ­¥é©Ÿï¼ŒåŒ…å« Ingress æŽ§åˆ¶å™¨ã€ç›£æŽ§ç³»çµ±ã€æ—¥èªŒç®¡ç†ã€åˆ†æ•£å¼è¿½è¹¤å’Œç®¡ç†å·¥å…·ï¼Œç¢ºä¿æŒ‰æ­£ç¢ºé †åºéƒ¨ç½²ä¸¦é…ç½®å›ºå®š IP åœ°å€ã€‚

## ç’°å¢ƒè³‡è¨Š
- **Kubernetes å¢é›†**: hcch-k8s
- **ç¯€é»žæ•¸é‡**: 5 å€‹ï¼ˆ3 Master + 2 Workerï¼‰
- **å®‰è£é †åº**: Nginx Ingress â†’ Istio â†’ Prometheus â†’ Grafana â†’ Jaeger â†’ Kiali â†’ Elasticsearch/Kibana â†’ K8s Dashboard â†’ Swagger UI

## IP åœ°å€åˆ†é…
| æœå‹™ | IP åœ°å€ | ç«¯å£ | ç”¨é€” |
|------|---------|------|------|
| Nginx Ingress | 172.21.169.73 | 80/443 | HTTP/HTTPS è·¯ç”± |
| Istio Ingress | 172.21.169.72 | 80/443 | æœå‹™ç¶²æ ¼å…¥å£ |
| Prometheus | 172.21.169.75 | 9090 | ç›£æŽ§æ•¸æ“šæ”¶é›† |
| Grafana | 172.21.169.74 | 3000 | ç›£æŽ§è¦–è¦ºåŒ– |
| Jaeger UI | 172.21.169.82 | 16686 | åˆ†æ•£å¼è¿½è¹¤ |
| Kiali | 172.21.169.77 | 20001 | æœå‹™ç¶²æ ¼è¦–è¦ºåŒ– |
| Kibana | 172.21.169.71 | 5601 | æ—¥èªŒæœå°‹å’Œè¦–è¦ºåŒ– |
| K8s Dashboard | 172.21.169.81 | 8443 | å¢é›†ç®¡ç†ç•Œé¢ |
| Swagger UI | 172.21.169.79 | 8080 | API æ–‡ä»¶å’Œæ¸¬è©¦ |

---

# ç¬¬äºŒéšŽæ®µï¼šIngress æŽ§åˆ¶å™¨

## 1. Nginx Ingress Controller å®‰è£ (172.21.169.73)

### 1.1 å‰ç½®æ¢ä»¶æª¢æŸ¥
```bash
# æª¢æŸ¥å¢é›†ç‹€æ…‹
kubectl get nodes
kubectl get pods -A | grep -E "(Running|Ready)"

# æª¢æŸ¥ Helm
helm version

# æª¢æŸ¥ç›®æ¨™ IP å¯ç”¨æ€§
ping -c 3 172.21.169.73  # æ‡‰è©²ç„¡æ³•é€£é€š
```

### 1.2 å®‰è£ MetalLBï¼ˆå¦‚æœªå®‰è£ï¼‰
```bash
# æª¢æŸ¥æ˜¯å¦å·²å®‰è£
kubectl get namespace metallb-system

# å¦‚æžœä¸å­˜åœ¨å‰‡å®‰è£
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.12/config/manifests/metallb-native.yaml

# ç­‰å¾…å•Ÿå‹•
kubectl wait --namespace metallb-system \
  --for=condition=ready pod \
  --selector=app=metallb \
  --timeout=90s
```

### 1.3 é…ç½® MetalLB IP åœ°å€æ± 
```bash
cat > metallb-nginx-config.yaml << 'EOF'
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: nginx-ingress-pool
  namespace: metallb-system
spec:
  addresses:
  - 172.21.169.73/32
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: nginx-ingress-advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
  - nginx-ingress-pool
EOF

kubectl apply -f metallb-nginx-config.yaml
```

### 1.4 æ–°å¢ž Nginx Ingress Helm Repository
```bash
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
```

### 1.5 å‰µå»º Nginx Values é…ç½®
```bash
cat > nginx-values.yaml << 'EOF'
controller:
  replicaCount: 2
  
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 256Mi
  
  service:
    type: LoadBalancer
    loadBalancerIP: "172.21.169.73"
    externalTrafficPolicy: Local
    annotations:
      metallb.universe.tf/loadBalancerIPs: "172.21.169.73"
  
  ingressClassResource:
    name: nginx
    enabled: true
    default: true
    controllerValue: "k8s.io/ingress-nginx"
  
  config:
    log-format-json: "true"
    worker-processes: "auto"
    worker-connections: "2048"
    ssl-protocols: "TLSv1.2 TLSv1.3"
    client-max-body-size: "100m"
    proxy-connect-timeout: "60"
    proxy-send-timeout: "60"
    proxy-read-timeout: "60"
    use-forwarded-headers: "true"
    compute-full-forwarded-for: "true"

  metrics:
    enabled: true
    service:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "10254"

defaultBackend:
  enabled: true
  replicaCount: 1
  resources:
    limits:
      cpu: 10m
      memory: 20Mi
    requests:
      cpu: 10m
      memory: 20Mi
EOF
```

### 1.6 å®‰è£ Nginx Ingress
```bash
helm install nginx-ingress ingress-nginx/ingress-nginx \
  --namespace ingress-nginx \
  --create-namespace \
  --values nginx-values.yaml \
  --version 4.11.3
```

### 1.7 é©—è­‰ Nginx Ingress å®‰è£
```bash
# æª¢æŸ¥ Pods
kubectl get pods -n ingress-nginx

# æª¢æŸ¥ Service å’Œ IP
kubectl get svc -n ingress-nginx

# æ¸¬è©¦é€£é€šæ€§
curl -I http://172.21.169.73
```

**é æœŸçµæžœ**ï¼š
```
NAME                                 TYPE           EXTERNAL-IP      PORT(S)
nginx-ingress-ingress-nginx-controller   LoadBalancer   172.21.169.73    80:xxxxx/TCP,443:xxxxx/TCP
```

---

## 2. Istio Ingress Gateway å®‰è£ (172.21.169.72)

### 2.1 å®‰è£ Istio CLI
```bash
# ä¸‹è¼‰ Istio
curl -L https://istio.io/downloadIstio | sh -

# ç§»å‹•åˆ° PATH
sudo mv istio-*/bin/istioctl /usr/local/bin/

# é©—è­‰å®‰è£
istioctl version
```

### 2.2 å®‰è£ Istio Control Plane
```bash
# ä½¿ç”¨é è¨­é…ç½®å®‰è£
istioctl install --set values.defaultRevision=default -y

# æˆ–ä½¿ç”¨è‡ªå®šç¾©é…ç½®
cat > istio-control-plane.yaml << 'EOF'
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: control-plane
spec:
  values:
    defaultRevision: default
    pilot:
      traceSampling: 1.0
  components:
    pilot:
      k8s:
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
EOF

istioctl install -f istio-control-plane.yaml -y
```

### 2.3 é…ç½® MetalLB ç‚º Istio åˆ†é… IP
```bash
cat > metallb-istio-config.yaml << 'EOF'
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: istio-ingress-pool
  namespace: metallb-system
spec:
  addresses:
  - 172.21.169.72/32
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: istio-ingress-advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
  - istio-ingress-pool
EOF

kubectl apply -f metallb-istio-config.yaml
```

### 2.4 å®‰è£ Istio Ingress Gateway
```bash
# å‰µå»º Istio Gateway é…ç½®
cat > istio-gateway.yaml << 'EOF'
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: ingress-gateway
spec:
  components:
    ingressGateways:
    - name: istio-ingressgateway
      namespace: istio-system
      enabled: true
      k8s:
        service:
          type: LoadBalancer
          loadBalancerIP: "172.21.169.72"
        overlays:
        - apiVersion: v1
          kind: Service
          name: istio-ingressgateway
          patches:
          - path: metadata.annotations
            value:
              metallb.universe.tf/loadBalancerIPs: "172.21.169.72"
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
        hpaSpec:
          minReplicas: 2
          maxReplicas: 5
EOF

istioctl install -f istio-gateway.yaml -y
```

### 2.5 é©—è­‰ Istio å®‰è£
```bash
# æª¢æŸ¥ Control Plane
kubectl get pods -n istio-system

# æª¢æŸ¥ Gateway Service
kubectl get svc -n istio-system istio-ingressgateway

# æ¸¬è©¦é€£é€šæ€§
curl -I http://172.21.169.72
```

### 2.6 å‰µå»ºæ¸¬è©¦ Gateway å’Œ VirtualService
```bash
cat > istio-test.yaml << 'EOF'
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: test-gateway
  namespace: istio-system
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"
---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: test-vs
  namespace: istio-system
spec:
  hosts:
  - "*"
  gateways:
  - test-gateway
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        host: nginx-ingress-ingress-nginx-controller.ingress-nginx.svc.cluster.local
        port:
          number: 80
EOF

kubectl apply -f istio-test.yaml
```

---

# ç¬¬ä¸‰éšŽæ®µï¼šç›£æŽ§å’Œå¯è§€æ¸¬æ€§

## 3. Prometheus å®‰è£ (172.21.169.75:9090)

### 3.1 æ–°å¢ž Prometheus Helm Repository
```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
```

### 3.2 é…ç½® MetalLB ç‚º Prometheus åˆ†é… IP
```bash
cat > metallb-prometheus-config.yaml << 'EOF'
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: prometheus-pool
  namespace: metallb-system
spec:
  addresses:
  - 172.21.169.75/32
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: prometheus-advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
  - prometheus-pool
EOF

kubectl apply -f metallb-prometheus-config.yaml
```

### 3.3 å‰µå»º Prometheus Values é…ç½®

**âš ï¸ å¯¦éš›éƒ¨ç½²èªªæ˜Ž**: ä»¥ä¸‹é…ç½®ä¸­çš„ `server` éƒ¨åˆ†å° kube-prometheus-stack ç„¡æ•ˆï¼Œæœƒå°Žè‡´ LoadBalancer æœå‹™ç„¡æ³•å‰µå»ºã€‚è«‹æŒ‰ç…§å¾ŒçºŒçš„ä¿®å¾©æ­¥é©Ÿæ“ä½œã€‚

```bash
cat > prometheus-values.yaml << 'EOF'
prometheus:
  prometheusSpec:
    resources:
      requests:
        memory: 1Gi
        cpu: 500m
      limits:
        memory: 2Gi
        cpu: 1000m
    
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: nfs-storage
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    
    retention: 30d
    retentionSize: 45GB
    
    externalUrl: http://172.21.169.75:9090
    
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false
    
    additionalScrapeConfigs:
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__

server:
  service:
    type: LoadBalancer
    loadBalancerIP: "172.21.169.75"
    annotations:
      metallb.universe.tf/loadBalancerIPs: "172.21.169.75"
    port: 9090

alertmanager:
  enabled: true
  alertmanagerSpec:
    resources:
      requests:
        memory: 256Mi
        cpu: 100m
      limits:
        memory: 512Mi
        cpu: 200m
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: nfs-storage
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi

grafana:
  enabled: false

nodeExporter:
  enabled: true

kubeStateMetrics:
  enabled: true

defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    configReloaders: true
    general: true
    k8s: true
    kubeApiserverAvailability: true
    kubeApiserverBurnrate: true
    kubeApiserverHistogram: true
    kubeApiserverSlos: true
    kubelet: true
    kubeProxy: true
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    network: true
    node: true
    nodeExporterAlerting: true
    nodeExporterRecording: true
    prometheus: true
    prometheusOperator: true

rbac:
  create: true
EOF
```

### 3.4 å®‰è£ Prometheus Stack
```bash
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --values prometheus-values.yaml \
  --version 61.1.1
```

### 3.5 é©—è­‰ Prometheus å®‰è£
```bash
# æª¢æŸ¥ Pods
kubectl get pods -n monitoring

# æª¢æŸ¥ Service
kubectl get svc -n monitoring | grep prometheus

# æª¢æŸ¥ PVC
kubectl get pvc -n monitoring

# æ¸¬è©¦è¨ªå•
curl http://172.21.169.75:9090
```

### 3.6 LoadBalancer å•é¡Œä¿®å¾©

**âš ï¸ é‡è¦ï¼šå¯¦éš›éƒ¨ç½²ç¶“é©—**

ä½¿ç”¨ä¸Šè¿°é…ç½®å®‰è£å¾Œï¼ŒPrometheus æœå‹™åªæœƒå‰µå»ºç‚º ClusterIP é¡žåž‹ï¼Œç„¡æ³•é€šéŽ LoadBalancer IP è¨ªå•ã€‚é€™æ˜¯å› ç‚º `server` é…ç½®å° `kube-prometheus-stack` ç„¡æ•ˆã€‚

#### å•é¡Œç¾è±¡ï¼š
```bash
# æª¢æŸ¥æœå‹™æœƒç™¼ç¾åªæœ‰ ClusterIP é¡žåž‹
systex@hcch-k8s-ms01:~$ kubectl get svc -n monitoring | grep prometheus
prometheus-kube-prometheus-prometheus     ClusterIP   10.96.4.48      <none>        9090/TCP,8080/TCP

# æ¸¬è©¦è¨ªå•æœƒå¤±æ•—
systex@hcch-k8s-ms01:~$ curl http://172.21.169.75:9090
curl: (7) Failed to connect to 172.21.169.75 port 9090: No route to host
```

#### è§£æ±ºæ–¹æ¡ˆï¼ˆå·²é©—è­‰æœ‰æ•ˆï¼‰ï¼š

**æ–¹æ³•ä¸€ï¼šæ‰‹å‹•å‰µå»º LoadBalancer æœå‹™ï¼ˆæŽ¨è–¦ï¼‰**
```bash
cat > prometheus-loadbalancer.yaml << 'EOF'
apiVersion: v1
kind: Service
metadata:
  name: prometheus-external
  namespace: monitoring
  annotations:
    metallb.universe.tf/loadBalancerIPs: "172.21.169.75"
spec:
  type: LoadBalancer
  loadBalancerIP: "172.21.169.75"
  ports:
  - name: web
    port: 9090
    targetPort: 9090
    protocol: TCP
  selector:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: prometheus-kube-prometheus-prometheus
EOF

kubectl apply -f prometheus-loadbalancer.yaml
```

**æ–¹æ³•äºŒï¼šä½¿ç”¨æ­£ç¢ºçš„ Helm é…ç½®ï¼ˆå®Œæ•´ä¿®å¾©ï¼‰**
```bash
# å¦‚æžœé¸æ“‡é‡æ–°å®‰è£ï¼Œè«‹ä¿®æ”¹ prometheus-values.yamlï¼š
# åˆªé™¤åŽŸå§‹é…ç½®ä¸­çš„ server éƒ¨åˆ†ï¼Œæ”¹ç‚ºï¼š
prometheus:
  service:
    type: LoadBalancer
    loadBalancerIP: "172.21.169.75"
    annotations:
      metallb.universe.tf/loadBalancerIPs: "172.21.169.75"
    port: 9090
```

#### é©—è­‰ä¿®å¾©ï¼š
```bash
# æª¢æŸ¥æ–°æœå‹™
kubectl get svc -n monitoring prometheus-external

# é æœŸçµæžœï¼š
# NAME                  TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE
# prometheus-external   LoadBalancer   10.96.xxx.xxx   172.21.169.75   9090:xxxxx/TCP   30s

# æ¸¬è©¦è¨ªå•
curl -I http://172.21.169.75:9090
# é æœŸè¿”å›žï¼šHTTP/1.1 200 OK

# æª¢æŸ¥ç›£æŽ§ç›®æ¨™
curl -s http://172.21.169.75:9090/api/v1/targets | jq '.data.activeTargets[] | .health' | grep up
```

### 3.7 é…ç½® Nginx Ingress è¦å‰‡ï¼ˆå¯é¸ï¼‰
```bash
cat > prometheus-ingress.yaml << 'EOF'
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: prometheus-ingress
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: prometheus.172.21.169.73.nip.io
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: prometheus-external
            port:
              number: 9090
EOF

kubectl apply -f prometheus-ingress.yaml
```

---

## 4. Grafana å®‰è£ (172.21.169.74)

### 4.1 é…ç½® MetalLB ç‚º Grafana åˆ†é… IP
```bash
cat > metallb-grafana-config.yaml << 'EOF'
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: grafana-pool
  namespace: metallb-system
spec:
  addresses:
  - 172.21.169.74/32
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: grafana-advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
  - grafana-pool
EOF

kubectl apply -f metallb-grafana-config.yaml
```

### 4.2 æ–°å¢ž Grafana Helm Repository
```bash
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
```

### 4.3 å‰µå»º Grafana Values é…ç½®
```bash
cat > grafana-values.yaml << 'EOF'
replicas: 1

resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 256Mi

persistence:
  enabled: true
  storageClassName: nfs-storage
  size: 10Gi
  accessModes:
    - ReadWriteOnce

service:
  type: LoadBalancer
  loadBalancerIP: "172.21.169.74"
  port: 80
  targetPort: 3000
  annotations:
    metallb.universe.tf/loadBalancerIPs: "172.21.169.74"

adminUser: admin
adminPassword: Grafana123!

grafana.ini:
  server:
    root_url: http://172.21.169.74
    serve_from_sub_path: false
  security:
    allow_embedding: true
    cookie_secure: false
  auth.anonymous:
    enabled: true
    org_role: Viewer
  dashboards:
    default_home_dashboard_path: /var/lib/grafana/dashboards/kubernetes-cluster-monitoring.json

datasources:
  datasources.yaml:
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      url: http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090
      access: proxy
      isDefault: true
      jsonData:
        timeInterval: 30s
        queryTimeout: 60s
        httpMethod: POST
    - name: Prometheus-AlertManager
      type: alertmanager
      url: http://prometheus-kube-prometheus-alertmanager.monitoring.svc.cluster.local:9093
      access: proxy

dashboardProviders:
  dashboardproviders.yaml:
    apiVersion: 1
    providers:
    - name: 'kubernetes'
      orgId: 1
      folder: 'Kubernetes'
      type: file
      disableDeletion: false
      editable: true
      allowUiUpdates: true
      options:
        path: /var/lib/grafana/dashboards/kubernetes
    - name: 'istio'
      orgId: 1
      folder: 'Istio'
      type: file
      disableDeletion: false
      editable: true
      allowUiUpdates: true
      options:
        path: /var/lib/grafana/dashboards/istio

dashboards:
  kubernetes:
    kubernetes-cluster-monitoring:
      gnetId: 7249
      revision: 1
      datasource: Prometheus
    node-exporter-full:
      gnetId: 1860
      revision: 31
      datasource: Prometheus
    kubernetes-deployment:
      gnetId: 8588
      revision: 1
      datasource: Prometheus
    kubernetes-pods:
      gnetId: 6336
      revision: 1
      datasource: Prometheus
  istio:
    # Istio æŽ§åˆ¶å¹³é¢å„€è¡¨æ¿
    istio-control-plane:
      gnetId: 7645
      revision: 75
      datasource: Prometheus
    istio-service:
      gnetId: 7636
      revision: 75
      datasource: Prometheus
    istio-workload:
      gnetId: 7630
      revision: 75
      datasource: Prometheus

plugins:
  - grafana-piechart-panel
  - grafana-worldmap-panel
  - grafana-clock-panel

env:
  GF_EXPLORE_ENABLED: true
  GF_PANELS_DISABLE_SANITIZE_HTML: true
  GF_LOG_FILTERS: rendering:debug

rbac:
  create: true
  pspEnabled: false

serviceAccount:
  create: true

securityContext:
  runAsUser: 472
  runAsGroup: 472
  fsGroup: 472

initChownData:
  enabled: true
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi
EOF
```

### 4.4 å®‰è£ Grafana
```bash
helm install grafana grafana/grafana \
  --namespace monitoring \
  --values grafana-values.yaml \
  --version 8.5.1
```

### 4.5 é©—è­‰ Grafana å®‰è£
```bash
# æª¢æŸ¥ Pod
kubectl get pods -n monitoring | grep grafana

# æª¢æŸ¥ Service
kubectl get svc -n monitoring | grep grafana

# æª¢æŸ¥ PVC
kubectl get pvc -n monitoring | grep grafana

# æ¸¬è©¦è¨ªå•
curl -I http://172.21.169.74
```

### 4.6 ç²å– Grafana ç™»å…¥è³‡è¨Š
```bash
# ç²å–ç®¡ç†å“¡å¯†ç¢¼ï¼ˆå¦‚æžœä½¿ç”¨è‡ªå‹•ç”Ÿæˆï¼‰
kubectl get secret --namespace monitoring grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo

# æˆ–ä½¿ç”¨é…ç½®ä¸­è¨­å®šçš„å¯†ç¢¼
echo "Username: admin"
echo "Password: Grafana123!"
```

### 4.7 é…ç½® Nginx Ingress è¦å‰‡ï¼ˆå¯é¸ï¼‰
```bash
cat > grafana-ingress.yaml << 'EOF'
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana-ingress
  namespace: monitoring
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: grafana.172.21.169.73.nip.io
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: grafana
            port:
              number: 80
EOF

kubectl apply -f grafana-ingress.yaml
```

---

# æ•´é«”é©—è­‰å’Œæ¸¬è©¦

## 1. æª¢æŸ¥æ‰€æœ‰æœå‹™ç‹€æ…‹
```bash
# æª¢æŸ¥æ‰€æœ‰å‘½åç©ºé–“çš„ Pods
kubectl get pods -A | grep -E "(nginx|istio|prometheus|grafana)"

# æª¢æŸ¥æ‰€æœ‰ LoadBalancer Services
kubectl get svc -A | grep LoadBalancer

# æª¢æŸ¥ MetalLB IP åˆ†é…
kubectl get svc -A -o wide | grep -E "(172.21.169.7[2-5])"
```

## 2. é€£é€šæ€§æ¸¬è©¦
```bash
# æ¸¬è©¦æ‰€æœ‰æœå‹™
echo "Testing Nginx Ingress..."
curl -I http://172.21.169.73

echo "Testing Istio Ingress..."
curl -I http://172.21.169.72

echo "Testing Prometheus..."
curl -I http://172.21.169.75:9090

echo "Testing Grafana..."
curl -I http://172.21.169.74
```

## 3. åŠŸèƒ½é©—è­‰
```bash
# æª¢æŸ¥ Prometheus targets
curl -s http://172.21.169.75:9090/api/v1/targets | jq '.data.activeTargets[].labels.job' | sort | uniq

# æª¢æŸ¥ Grafana datasources
curl -s -u admin:Grafana123! http://172.21.169.74/api/datasources | jq '.[].name'
```

## 4. éƒ¨ç½²æ¸¬è©¦æ‡‰ç”¨
```bash
cat > test-microservice.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-app
  namespace: default
  labels:
    app: test-app
    version: v1
spec:
  replicas: 2
  selector:
    matchLabels:
      app: test-app
      version: v1
  template:
    metadata:
      labels:
        app: test-app
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: test-app
        image: nginx:1.21
        ports:
        - containerPort: 80
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: test-app-service
  namespace: default
  labels:
    app: test-app
spec:
  selector:
    app: test-app
  ports:
  - name: http
    port: 80
    targetPort: 80
  - name: metrics
    port: 8080
    targetPort: 8080
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-app-nginx-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: test-nginx.172.21.169.73.nip.io
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: test-app-service
            port:
              number: 80
---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: test-app-istio-vs
  namespace: default
spec:
  hosts:
  - test-istio.172.21.169.72.nip.io
  gateways:
  - istio-system/test-gateway
  http:
  - route:
    - destination:
        host: test-app-service
        port:
          number: 80
EOF

kubectl apply -f test-microservice.yaml
```

## 5. æ¸¬è©¦å®Œæ•´æµç¨‹
```bash
# æ¸¬è©¦ Nginx Ingress è·¯ç”±
curl -H "Host: test-nginx.172.21.169.73.nip.io" http://172.21.169.73

# æ¸¬è©¦ Istio Ingress è·¯ç”±  
curl -H "Host: test-istio.172.21.169.72.nip.io" http://172.21.169.72

# æª¢æŸ¥ Prometheus ä¸­çš„ç›®æ¨™
curl -s http://172.21.169.75:9090/api/v1/targets | jq '.data.activeTargets[] | select(.labels.job=="kubernetes-pods")'

# è¨ªå• Grafana å„€è¡¨æ¿
echo "Open http://172.21.169.74 in browser"
echo "Login: admin / Grafana123!"
```

---

# æ•…éšœæŽ’é™¤

## å¸¸è¦‹å•é¡ŒåŠè§£æ±ºæ–¹æ¡ˆ

### 0. Prometheus LoadBalancer ç„¡æ³•è¨ªå•
```bash
# å•é¡Œï¼šä½¿ç”¨ kube-prometheus-stack æ™‚ LoadBalancer IP ç„¡æ³•è¨ªå•
# åŽŸå› ï¼šserver é…ç½®ç„¡æ•ˆï¼Œæœå‹™å‰µå»ºç‚º ClusterIP é¡žåž‹

# è¨ºæ–·æ­¥é©Ÿï¼š
kubectl get svc -n monitoring | grep prometheus
# å¦‚æžœåªé¡¯ç¤º ClusterIP é¡žåž‹ï¼Œå‰‡éœ€è¦æ‰‹å‹•å‰µå»º LoadBalancer æœå‹™

# è§£æ±ºæ–¹æ¡ˆï¼š
kubectl apply -f prometheus-loadbalancer.yaml  # ä½¿ç”¨ä¸Šè¿°ä¿®å¾©é…ç½®

# é©—è­‰ï¼š
kubectl get svc -n monitoring prometheus-external
curl -I http://172.21.169.75:9090
```

### 1. IP åœ°å€ç„¡æ³•åˆ†é…
```bash
# æª¢æŸ¥ MetalLB ç‹€æ…‹
kubectl get pods -n metallb-system
kubectl logs -n metallb-system deployment/controller

# æª¢æŸ¥ IP åœ°å€æ± 
kubectl get ipaddresspool -n metallb-system
```

### 2. Pod ç„¡æ³•å•Ÿå‹•
```bash
# æª¢æŸ¥ Pod äº‹ä»¶
kubectl describe pod <pod-name> -n <namespace>

# æª¢æŸ¥ç¯€é»žè³‡æº
kubectl top nodes
kubectl describe nodes
```

### 3. æŒä¹…åŒ–å­˜å„²å•é¡Œ
```bash
# æª¢æŸ¥ PVC ç‹€æ…‹
kubectl get pvc -A

# æª¢æŸ¥ StorageClass
kubectl get storageclass

# æª¢æŸ¥ NFS Provisioner
kubectl get pods -n nfs
```

### 4. æœå‹™é–“é€£é€šæ€§å•é¡Œ
```bash
# æª¢æŸ¥ DNS è§£æž
kubectl run test-pod --image=busybox --rm -it -- nslookup prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local

# æª¢æŸ¥ç¶²è·¯ç­–ç•¥
kubectl get networkpolicy -A
```

---

# æ¸…ç†æŒ‡ä»¤

å¦‚éœ€ç§»é™¤æ‰€æœ‰æœå‹™ï¼š
```bash
# åˆªé™¤æ¸¬è©¦æ‡‰ç”¨
kubectl delete -f test-microservice.yaml

# åˆªé™¤ Grafana
helm uninstall grafana -n monitoring

# åˆªé™¤ Prometheus
helm uninstall prometheus -n monitoring

# åˆªé™¤ Istio
istioctl uninstall --purge -y

# åˆªé™¤ Nginx Ingress
helm uninstall nginx-ingress -n ingress-nginx

# åˆªé™¤å‘½åç©ºé–“
kubectl delete namespace monitoring ingress-nginx istio-system

# æ¸…ç† MetalLB é…ç½®
kubectl delete -f metallb-*-config.yaml
```

# ç¬¬å››éšŽæ®µï¼šåˆ†æ•£å¼è¿½è¹¤

## 5. Jaeger å®‰è£ (172.21.169.82)

### 5.1 é…ç½® MetalLB ç‚º Jaeger åˆ†é… IP
```bash
cat > metallb-jaeger-config.yaml << 'EOF'
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: jaeger-pool
  namespace: metallb-system
spec:
  addresses:
  - 172.21.169.82/32
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: jaeger-advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
  - jaeger-pool
EOF

kubectl apply -f metallb-jaeger-config.yaml
```

### 5.2 å®‰è£ Jaeger Operator
```bash
# å‰µå»ºå‘½åç©ºé–“
kubectl create namespace observability

# å®‰è£ Jaeger Operator
kubectl create -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.57.0/jaeger-operator.yaml -n observability

# ç­‰å¾… Operator å•Ÿå‹•
kubectl wait --for=condition=available deployment/jaeger-operator -n observability --timeout=300s
```

### 5.3 å‰µå»º Jaeger å¯¦ä¾‹é…ç½®

**âš ï¸ å¯¦éš›éƒ¨ç½²ç¶“é©—**ï¼šåŽŸå§‹çš„ç”Ÿç”¢ç’°å¢ƒé…ç½®ï¼ˆä½¿ç”¨ Elasticsearchï¼‰æœƒå°Žè‡´ Pod CrashLoopBackOffï¼Œå› ç‚ºéœ€è¦é¡å¤–å®‰è£å’Œé…ç½® Elasticsearchã€‚æŽ¨è–¦ä½¿ç”¨å…§å­˜å­˜å„²ç‰ˆæœ¬ï¼Œç©©å®šä¸”å¿«é€Ÿå•Ÿå‹•ã€‚

#### 5.3.1 ä½¿ç”¨å…§å­˜å­˜å„²ç‰ˆæœ¬ï¼ˆæŽ¨è–¦ï¼‰

```bash
cat > jaeger-memory-production.yaml << 'EOF'
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: jaeger-production
  namespace: observability
spec:
  strategy: allInOne
  allInOne:
    image: jaegertracing/all-in-one:1.57
    resources:
      requests:
        cpu: 300m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 1Gi
    options:
      memory:
        max-traces: 100000
      query:
        base-path: /
      collector:
        zipkin:
          host-port: ":9411"
  storage:
    type: memory
EOF

kubectl apply -f jaeger-memory-production.yaml
```

#### 5.3.2 ç­‰å¾… Jaeger Pod å•Ÿå‹•

```bash
echo "ç­‰å¾… Jaeger Pod å•Ÿå‹•..."
kubectl wait --for=condition=ready pod -l app=jaeger -n observability --timeout=120s

echo "æª¢æŸ¥ Pod ç‹€æ…‹ï¼š"
kubectl get pods -n observability

echo "æª¢æŸ¥æœå‹™ï¼š"
kubectl get svc -n observability
```

#### 5.3.3 å‰µå»º LoadBalancer æœå‹™

**âš ï¸ é‡è¦**ï¼šJaeger Operator ä¸æ”¯æŒåœ¨ CRD ä¸­ç›´æŽ¥é…ç½® LoadBalancer æœå‹™ï¼Œéœ€è¦æ‰‹å‹•å‰µå»ºå¤–éƒ¨æœå‹™ã€‚

```bash
cat > jaeger-loadbalancer-service.yaml << 'EOF'
apiVersion: v1
kind: Service
metadata:
  name: jaeger-external
  namespace: observability
  annotations:
    metallb.universe.tf/loadBalancerIPs: "172.21.169.82"
spec:
  type: LoadBalancer
  loadBalancerIP: "172.21.169.82"
  ports:
  - name: query-http
    port: 16686
    targetPort: 16686
    protocol: TCP
  - name: collector-grpc
    port: 14250
    targetPort: 14250
    protocol: TCP
  - name: collector-http
    port: 14268
    targetPort: 14268
    protocol: TCP
  - name: zipkin
    port: 9411
    targetPort: 9411
    protocol: TCP
  - name: admin
    port: 14269
    targetPort: 14269
    protocol: TCP
  selector:
    app.kubernetes.io/name: jaeger-production
    app.kubernetes.io/component: all-in-one
EOF

kubectl apply -f jaeger-loadbalancer-service.yaml
```

#### 5.3.4 ç­‰å¾… LoadBalancer IP åˆ†é…

```bash
echo "ç­‰å¾… LoadBalancer IP åˆ†é…..."
kubectl wait --for=jsonpath='{.status.loadBalancer.ingress[0].ip}' service/jaeger-external -n observability --timeout=60s

echo "æª¢æŸ¥ LoadBalancer ç‹€æ…‹ï¼š"
kubectl get svc jaeger-external -n observability
```

### 5.4 LoadBalancer æœå‹™å•é¡Œä¿®å¾©

**âš ï¸ å¸¸è¦‹å•é¡Œ**ï¼šå¦‚æžœå‰µå»º LoadBalancer æœå‹™å¾Œç„¡æ³•è¨ªå• Jaeger UIï¼Œé€šå¸¸æ˜¯æœå‹™é¸æ“‡å™¨èˆ‡ Pod æ¨™ç±¤ä¸åŒ¹é…å°Žè‡´çš„ã€‚

#### 5.4.1 è¨ºæ–·æ­¥é©Ÿ

```bash
echo "=== æª¢æŸ¥ Pod å¯¦éš›æ¨™ç±¤ ==="
kubectl get pods -n observability --show-labels | grep jaeger

echo "=== æª¢æŸ¥æœå‹™ç«¯é»ž ==="
kubectl get endpoints -n observability | grep jaeger

echo "=== æ¸¬è©¦é€£é€šæ€§ ==="
curl -I http://172.21.169.82:16686
```

#### 5.4.2 ä¿®å¾©æœå‹™é¸æ“‡å™¨ï¼ˆå¦‚æœ‰éœ€è¦ï¼‰

```bash
# å¦‚æžœä¸Šè¿°æ¸¬è©¦å¤±æ•—ï¼Œæ›´æ–°æœå‹™é¸æ“‡å™¨
kubectl patch svc jaeger-external -n observability -p '{
  "spec": {
    "selector": {
      "app.kubernetes.io/name": "jaeger-production"
    }
  }
}'

# ç­‰å¾…ç«¯é»žæ›´æ–°
sleep 5

# é‡æ–°æ¸¬è©¦
curl -I http://172.21.169.82:16686
```

### 5.5 é…ç½® Istio èˆ‡ Jaeger æ•´åˆ

```bash
cat > istio-jaeger-tracing.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: istio
  namespace: istio-system
data:
  mesh: |
    defaultConfig:
      proxyStatsMatcher:
        inclusionRegexps:
        - ".*outlier_detection.*"
        - ".*circuit_breakers.*"
        - ".*upstream_rq_retry.*"
        - ".*_cx_.*"
      tracing:
        zipkin:
          address: jaeger-production-collector.observability.svc.cluster.local:9411
    defaultProviders:
      tracing:
      - jaeger
    extensionProviders:
    - name: jaeger
      zipkin:
        service: jaeger-production-collector.observability.svc.cluster.local
        port: 9411
EOF

kubectl apply -f istio-jaeger-tracing.yaml

# é‡å•Ÿ Istio æŽ§åˆ¶å¹³é¢
kubectl rollout restart deployment/istiod -n istio-system

echo "ç­‰å¾… Istio é‡å•Ÿå®Œæˆ..."
kubectl wait --for=condition=available deployment/istiod -n istio-system --timeout=120s
```

### 5.6 é©—è­‰ Jaeger å®‰è£

```bash
echo "=== é©—è­‰ Jaeger éƒ¨ç½²ç‹€æ…‹ ==="

echo "1. Pod ç‹€æ…‹ï¼š"
kubectl get pods -n observability

echo "2. æœå‹™ç‹€æ…‹ï¼š"
kubectl get svc -n observability

echo "3. LoadBalancer IPï¼š"
kubectl get svc jaeger-external -n observability -o jsonpath='{.status.loadBalancer.ingress[0].ip}'
echo
```

### 5.7 æ¸¬è©¦é€£é€šæ€§å’ŒåŠŸèƒ½

```bash
echo "=== æ¸¬è©¦æ‰€æœ‰ç«¯é»žé€£é€šæ€§ ==="

echo "æ¸¬è©¦ Jaeger UI (16686)..."
curl -I http://172.21.169.82:16686

echo "æ¸¬è©¦æ”¶é›†å™¨ HTTP (14268)..."
curl -I http://172.21.169.82:14268

echo "æ¸¬è©¦ Zipkin (9411)..."
curl -I http://172.21.169.82:9411

echo "æ¸¬è©¦ç®¡ç†ç«¯é»ž (14269)..."
curl -s http://172.21.169.82:14269/metrics | head -3

echo "æ¸¬è©¦ API..."
curl -s http://172.21.169.82:16686/api/services

echo ""
echo "ç€è¦½å™¨è¨ªå•: http://172.21.169.82:16686"
```

### 5.8 éƒ¨ç½²æ¸¬è©¦æ‡‰ç”¨é©—è­‰è¿½è¹¤åŠŸèƒ½

```bash
echo "=== éƒ¨ç½²æ¸¬è©¦æ‡‰ç”¨ ==="

cat > jaeger-test-apps.yaml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: productpage
  namespace: default
  labels:
    app: productpage
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: productpage
      version: v1
  template:
    metadata:
      labels:
        app: productpage
        version: v1
    spec:
      containers:
      - name: productpage
        image: nginx:1.21
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: productpage
  namespace: default
spec:
  ports:
  - port: 9080
    targetPort: 80
    name: http
  selector:
    app: productpage
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: details
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: details
  template:
    metadata:
      labels:
        app: details
    spec:
      containers:
      - name: details
        image: httpd:2.4
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: details
  namespace: default
spec:
  ports:
  - port: 9080
    targetPort: 80
    name: http
  selector:
    app: details
EOF

kubectl apply -f jaeger-test-apps.yaml

# ç‚º default å‘½åç©ºé–“å•Ÿç”¨ Istio æ³¨å…¥
kubectl label namespace default istio-injection=enabled --overwrite

# é‡å•Ÿæ‡‰ç”¨ä»¥æ³¨å…¥ sidecar
kubectl rollout restart deployment/productpage -n default
kubectl rollout restart deployment/details -n default

# ç­‰å¾…æ‡‰ç”¨é‡å•Ÿ
kubectl wait --for=condition=ready pod -l app=productpage -n default --timeout=60s
kubectl wait --for=condition=ready pod -l app=details -n default --timeout=60s
```

### 5.9 ç”Ÿæˆæ¸¬è©¦æµé‡

```bash
echo "=== ç”Ÿæˆæ¸¬è©¦æµé‡ ==="

# ç­‰å¾… sidecar å°±ç·’
sleep 10

echo "ç”Ÿæˆæ¸¬è©¦è«‹æ±‚..."
for i in {1..20}; do
  echo "ç™¼é€è«‹æ±‚ $i..."
  kubectl exec -n default deployment/productpage -- curl -s details:9080/ > /dev/null
  sleep 1
done

echo "æ¸¬è©¦æµé‡ç”Ÿæˆå®Œæˆï¼"

# æª¢æŸ¥è¿½è¹¤æ•¸æ“š
echo "ç­‰å¾…è¿½è¹¤æ•¸æ“šå‚³è¼¸..."
sleep 10

echo "æª¢æŸ¥ Jaeger ä¸­çš„æœå‹™ï¼š"
curl -s http://172.21.169.82:16686/api/services | jq -r '.data[]' 2>/dev/null || curl -s http://172.21.169.82:16686/api/services
```

### 5.10 é…ç½® Nginx Ingress è¦å‰‡ï¼ˆå¯é¸ï¼‰
```bash
cat > jaeger-ingress.yaml << 'EOF'
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: jaeger-ingress
  namespace: observability
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: jaeger.172.21.169.73.nip.io
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: jaeger-external
            port:
              number: 16686
EOF

kubectl apply -f jaeger-ingress.yaml

# æ¸¬è©¦ Ingress è¨ªå•
echo "æ¸¬è©¦ Nginx Ingress è¨ªå•ï¼š"
curl -H "Host: jaeger.172.21.169.73.nip.io" http://172.21.169.73
```

### 5.11 Jaeger éƒ¨ç½²ç¶“é©—ç¸½çµ

**âœ… æˆåŠŸéƒ¨ç½²æŒ‡æ¨™**ï¼š
- Jaeger Pod ç‹€æ…‹ç‚º `1/1 Running`
- LoadBalancer æœå‹™åˆ†é…åˆ° `172.21.169.82`
- å¯ä»¥è¨ªå• `http://172.21.169.82:16686`
- æ¸¬è©¦æ‡‰ç”¨ç”¢ç”Ÿè¿½è¹¤æ•¸æ“šå¯è¦‹
- Jaeger UI ä¸­å¯ä»¥çœ‹åˆ°æœå‹™åˆ—è¡¨

**âš ï¸ å¯¦éš›éƒ¨ç½²ç¶“é©—**ï¼š
1. **å­˜å„²é¸æ“‡**ï¼šå…§å­˜å­˜å„²æ¯” Elasticsearch æ›´ç©©å®šï¼Œé©åˆæ¸¬è©¦å’Œä¸­å°è¦æ¨¡éƒ¨ç½²
2. **æœå‹™é…ç½®**ï¼šLoadBalancer æœå‹™å¿…é ˆæ‰‹å‹•å‰µå»ºï¼ŒCRD ä¸æ”¯æŒç›´æŽ¥é…ç½®
3. **æ¨™ç±¤åŒ¹é…**ï¼šæœå‹™é¸æ“‡å™¨å¿…é ˆèˆ‡ Pod æ¨™ç±¤ç²¾ç¢ºåŒ¹é…
4. **Istio æ•´åˆ**ï¼šéœ€è¦æ­£ç¢ºé…ç½® Zipkin ç«¯é»žæ‰èƒ½æ”¶é›†è¿½è¹¤æ•¸æ“š
5. **æ¸¬è©¦é‡è¦æ€§**ï¼šéƒ¨ç½²æ¸¬è©¦æ‡‰ç”¨æ˜¯é©—è­‰åŠŸèƒ½çš„é—œéµæ­¥é©Ÿ

**ðŸš¨ å¸¸è¦‹å•é¡Œå’Œè§£æ±ºæ–¹æ¡ˆ**ï¼š
- Pod CrashLoopBackOff â†’ ä½¿ç”¨å…§å­˜å­˜å„²æ›¿ä»£ Elasticsearch
- UI ç„¡æ³•è¨ªå• â†’ æª¢æŸ¥æœå‹™é¸æ“‡å™¨å’Œç«¯é»žç‹€æ…‹
- æ²’æœ‰è¿½è¹¤æ•¸æ“š â†’ ç¢ºèª Istio sidecar æ³¨å…¥å’Œé…ç½®
- LoadBalancer IP æœªåˆ†é… â†’ æª¢æŸ¥ MetalLB ç‹€æ…‹å’Œ IP åœ°å€æ± 

---

# ç¬¬äº”éšŽæ®µï¼šæœå‹™ç¶²æ ¼å¯è§€æ¸¬æ€§

## 6. Kiali å®‰è£ (172.21.169.77:20001)

**âš ï¸ å‰ç½®æ¢ä»¶**ï¼šç¢ºä¿ Jaeger å·²æ­£å¸¸é‹è¡Œï¼Œå› ç‚º Kiali éœ€è¦èˆ‡ Jaeger æ•´åˆä¾†é¡¯ç¤ºè¿½è¹¤ä¿¡æ¯ã€‚

### 6.1 é…ç½® MetalLB ç‚º Kiali åˆ†é… IP
```bash
cat > metallb-kiali-config.yaml << 'EOF'
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kiali-pool
  namespace: metallb-system
spec:
  addresses:
  - 172.21.169.77/32
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: kiali-advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
  - kiali-pool
EOF

kubectl apply -f metallb-kiali-config.yaml
```

### 6.2 å®‰è£ Kiali Operator
```bash
# æ–°å¢ž Kiali Helm Repository
helm repo add kiali https://kiali.org/helm-charts
helm repo update

# å®‰è£ Kiali Operator
helm install \
  --namespace kiali-operator \
  --create-namespace \
  kiali-operator \
  kiali/kiali-operator \
  --version 1.86.0
```

### 6.3 å‰µå»º Kiali å¯¦ä¾‹é…ç½®
```bash
cat > kiali-instance.yaml << 'EOF'
apiVersion: kiali.io/v1alpha1
kind: Kiali
metadata:
  name: kiali
  namespace: istio-system
spec:
  installation_tag: "v1.86.0"
  
  auth:
    strategy: "anonymous"
  
  deployment:
    replicas: 1
    resources:
      requests:
        cpu: 200m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi
    
    service_type: "LoadBalancer"
    service_annotations:
      metallb.universe.tf/loadBalancerIPs: "172.21.169.77"
    load_balancer_ip: "172.21.169.77"
    
    http_port: 20001
    
  external_services:
    prometheus:
      url: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
      
    grafana:
      enabled: true
      url: "http://grafana.monitoring.svc.cluster.local"
      in_cluster_url: "http://grafana.monitoring.svc.cluster.local"
      
    tracing:
      enabled: true
      in_cluster_url: "http://jaeger-production-query.observability.svc.cluster.local:16686"
      url: "http://172.21.169.82:16686"
      
  istio_namespace: "istio-system"
  
  api:
    namespaces:
      exclude:
      - "kube-.*"
      - "openshift.*"
      - "metallb-system"
      - "nfs"
      
  server:
    web_root: "/"
    web_fqdn: "172.21.169.77"
    web_port: 20001
    
  kiali_feature_flags:
    certificates_information_indicators:
      enabled: true
    clustering:
      enabled: false
    disabled_features: []
    validations:
      ignore: ["KIA1301"]
EOF

kubectl apply -f kiali-instance.yaml
```

### 6.4 ç­‰å¾… Kiali éƒ¨ç½²å®Œæˆ
```bash
# ç­‰å¾… Kiali Pod å•Ÿå‹•
kubectl wait --for=condition=Ready pod -l app=kiali -n istio-system --timeout=300s

# æª¢æŸ¥ Kiali ç‹€æ…‹
kubectl get pods -n istio-system | grep kiali
kubectl get svc -n istio-system | grep kiali
```

### 6.5 é©—è­‰ Kiali å®‰è£
```bash
# æ¸¬è©¦ Kiali UI
curl -I http://172.21.169.77:20001

# æª¢æŸ¥ Kiali é…ç½®
kubectl get kiali -n istio-system kiali -o yaml
```

### 6.6 é…ç½® Istio Sidecar æ³¨å…¥
```bash
# ç‚ºæ¸¬è©¦å‘½åç©ºé–“å•Ÿç”¨ Istio æ³¨å…¥
kubectl label namespace default istio-injection=enabled

# é‡å•Ÿç¾æœ‰çš„ Pod ä»¥æ³¨å…¥ Sidecar
kubectl rollout restart deployment -n default
```

---

# ç¬¬å…­éšŽæ®µï¼šæ—¥èªŒç®¡ç†

## 7. Elasticsearch å’Œ Kibana å®‰è£ (172.21.169.71:5601)

### 7.1 é…ç½® MetalLB ç‚º Kibana åˆ†é… IP
```bash
cat > metallb-kibana-config.yaml << 'EOF'
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kibana-pool
  namespace: metallb-system
spec:
  addresses:
  - 172.21.169.71/32
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: kibana-advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
  - kibana-pool
EOF

kubectl apply -f metallb-kibana-config.yaml
```

### 7.2 æ–°å¢ž Elastic Helm Repository
```bash
helm repo add elastic https://helm.elastic.co
helm repo update
```

### 7.3 å®‰è£ Elasticsearch
```bash
cat > elasticsearch-values.yaml << 'EOF'
replicas: 1
minimumMasterNodes: 1

resources:
  requests:
    cpu: 500m
    memory: 1Gi
  limits:
    cpu: 1000m
    memory: 2Gi

esJavaOpts: "-Xmx1g -Xms1g"

persistence:
  enabled: true
  storageClass: "nfs-storage"
  accessModes:
    - ReadWriteOnce
  size: 30Gi

clusterName: "elasticsearch"
nodeGroup: "master"

esConfig:
  elasticsearch.yml: |
    xpack.security.enabled: false
    xpack.security.transport.ssl.enabled: false
    xpack.security.http.ssl.enabled: false

service:
  type: ClusterIP
  port: 9200

readinessProbe:
  failureThreshold: 3
  initialDelaySeconds: 10
  periodSeconds: 10
  successThreshold: 3
  timeoutSeconds: 5
EOF

helm install elasticsearch elastic/elasticsearch \
  --namespace logging \
  --create-namespace \
  --values elasticsearch-values.yaml \
  --version 8.5.1
```

### 7.4 å®‰è£ Kibana
```bash
cat > kibana-values.yaml << 'EOF'
elasticsearchHosts: "http://elasticsearch-master.logging.svc.cluster.local:9200"

replicas: 1

resources:
  requests:
    cpu: 500m
    memory: 1Gi
  limits:
    cpu: 1000m
    memory: 2Gi

service:
  type: LoadBalancer
  loadBalancerIP: "172.21.169.71"
  port: 5601
  annotations:
    metallb.universe.tf/loadBalancerIPs: "172.21.169.71"

kibanaConfig:
  kibana.yml: |
    server.host: "0.0.0.0"
    server.port: 5601
    elasticsearch.hosts: ["http://elasticsearch-master.logging.svc.cluster.local:9200"]
    server.publicBaseUrl: "http://172.21.169.71:5601"
    
    xpack.security.enabled: false
    xpack.encryptedSavedObjects.encryptionKey: "fhjskloppd678ehkdfdlliverpoolfcr"
    
    kibana.index: ".kibana"
    
    logging.dest: stdout
    logging.silent: false
    logging.quiet: false
    logging.verbose: true

extraEnvs:
  - name: "NODE_OPTIONS"
    value: "--max-old-space-size=1800"
  - name: "KIBANA_SYSTEM_PASSWORD"
    value: "kibana123"

healthCheckPath: "/app/kibana"

serverHost: "0.0.0.0"

lifecycle:
  preStop:
    exec:
      command: ["/bin/bash", "-c", "sleep 20"]
EOF

helm install kibana elastic/kibana \
  --namespace logging \
  --values kibana-values.yaml \
  --version 8.5.1
```

### 7.5 å®‰è£ Filebeatï¼ˆæ—¥èªŒæ”¶é›†å™¨ï¼‰
```bash
cat > filebeat-values.yaml << 'EOF'
deployment:
  replicas: 1

resources:
  requests:
    cpu: 100m
    memory: 100Mi
  limits:
    cpu: 200m
    memory: 200Mi

filebeatConfig:
  filebeat.yml: |
    filebeat.inputs:
    - type: container
      paths:
        - /var/log/containers/*.log
      processors:
        - add_kubernetes_metadata:
            host: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"
    
    output.elasticsearch:
      host: '${NODE_NAME}'
      hosts: ["http://elasticsearch-master.logging.svc.cluster.local:9200"]
      indices:
        - index: "filebeat-kubernetes-%{+yyyy.MM.dd}"
    
    setup.template.name: "filebeat-kubernetes"
    setup.template.pattern: "filebeat-kubernetes-*"
    setup.template.settings:
      index.number_of_shards: 1
      index.number_of_replicas: 0
    
    setup.kibana:
      host: "http://kibana-kibana.logging.svc.cluster.local:5601"

extraVolumes:
  - name: varlog
    hostPath:
      path: /var/log
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/docker/containers

extraVolumeMounts:
  - name: varlog
    mountPath: /var/log
    readOnly: true
  - name: varlibdockercontainers
    mountPath: /var/lib/docker/containers
    readOnly: true

extraEnvs:
  - name: NODE_NAME
    valueFrom:
      fieldRef:
        fieldPath: spec.nodeName
EOF

helm install filebeat elastic/filebeat \
  --namespace logging \
  --values filebeat-values.yaml \
  --version 8.5.1
```

### 7.6 é©—è­‰ ELK Stack å®‰è£
```bash
# æª¢æŸ¥æ‰€æœ‰çµ„ä»¶
kubectl get pods -n logging

# æª¢æŸ¥ Elasticsearch å¢é›†å¥åº·ç‹€æ…‹
kubectl exec -n logging elasticsearch-master-0 -- curl -s http://localhost:9200/_cluster/health?pretty

# æª¢æŸ¥ Kibana æœå‹™
kubectl get svc -n logging kibana-kibana

# æ¸¬è©¦ Kibana UI
curl -I http://172.21.169.71:5601
```

---

# ç¬¬ä¸ƒéšŽæ®µï¼šç®¡ç†å·¥å…·

## 8. Kubernetes Dashboard å®‰è£ (172.21.169.81)

### 8.1 é…ç½® MetalLB ç‚º Dashboard åˆ†é… IP
```bash
cat > metallb-dashboard-config.yaml << 'EOF'
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: dashboard-pool
  namespace: metallb-system
spec:
  addresses:
  - 172.21.169.81/32
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: dashboard-advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
  - dashboard-pool
EOF

kubectl apply -f metallb-dashboard-config.yaml
```

### 8.2 å®‰è£ Kubernetes Dashboard
```bash
# å®‰è£ Dashboard
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

# ä¿®æ”¹ Service ç‚º LoadBalancer
kubectl patch svc kubernetes-dashboard -n kubernetes-dashboard -p '{"spec":{"type":"LoadBalancer","loadBalancerIP":"172.21.169.81"}}'

# æ·»åŠ  MetalLB è¨»è§£
kubectl annotate svc kubernetes-dashboard -n kubernetes-dashboard metallb.universe.tf/loadBalancerIPs="172.21.169.81"
```

### 8.3 å‰µå»ºç®¡ç†å“¡ç”¨æˆ¶
```bash
cat > dashboard-admin.yaml << 'EOF'
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: v1
kind: Secret
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
  annotations:
    kubernetes.io/service-account.name: "admin-user"
type: kubernetes.io/service-account-token
EOF

kubectl apply -f dashboard-admin.yaml
```

### 8.4 ç²å–è¨ªå• Token
```bash
# ç²å–è¨ªå• Token
kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath={".data.token"} | base64 -d
echo
```

### 8.5 é…ç½® Nginx Ingressï¼ˆHTTPS è¨ªå•ï¼‰
```bash
cat > dashboard-ingress.yaml << 'EOF'
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: kubernetes-dashboard-ingress
  namespace: kubernetes-dashboard
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: dashboard.172.21.169.73.nip.io
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: kubernetes-dashboard
            port:
              number: 443
EOF

kubectl apply -f dashboard-ingress.yaml
```

### 8.6 é©—è­‰ Dashboard å®‰è£
```bash
# æª¢æŸ¥ Dashboard çµ„ä»¶
kubectl get pods -n kubernetes-dashboard

# æª¢æŸ¥æœå‹™
kubectl get svc -n kubernetes-dashboard

# æ¸¬è©¦ HTTPS è¨ªå•
curl -k -I https://172.21.169.81:8443
```

---

## 9. Swagger UI å®‰è£ (172.21.169.79)

### 9.1 é…ç½® MetalLB ç‚º Swagger UI åˆ†é… IP
```bash
cat > metallb-swagger-config.yaml << 'EOF'
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: swagger-pool
  namespace: metallb-system
spec:
  addresses:
  - 172.21.169.79/32
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: swagger-advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
  - swagger-pool
EOF

kubectl apply -f metallb-swagger-config.yaml
```

### 9.2 å‰µå»º Swagger UI éƒ¨ç½²
```bash
cat > swagger-ui.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-config
  namespace: default
data:
  swagger.yaml: |
    openapi: 3.0.0
    info:
      title: Kubernetes Cluster APIs
      description: API documentation for the Kubernetes cluster services
      version: 1.0.0
    servers:
      - url: http://172.21.169.73
        description: Nginx Ingress Gateway
      - url: http://172.21.169.72
        description: Istio Ingress Gateway
    paths:
      /health:
        get:
          summary: Health check endpoint
          responses:
            '200':
              description: Service is healthy
      /metrics:
        get:
          summary: Prometheus metrics endpoint
          responses:
            '200':
              description: Metrics data
    components:
      schemas:
        HealthCheck:
          type: object
          properties:
            status:
              type: string
              example: "healthy"
            timestamp:
              type: string
              format: date-time
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: swagger-ui
  namespace: default
  labels:
    app: swagger-ui
spec:
  replicas: 1
  selector:
    matchLabels:
      app: swagger-ui
  template:
    metadata:
      labels:
        app: swagger-ui
    spec:
      containers:
      - name: swagger-ui
        image: swaggerapi/swagger-ui:v5.9.0
        ports:
        - containerPort: 8080
        env:
        - name: SWAGGER_JSON_URL
          value: "/swagger.yaml"
        - name: BASE_URL
          value: "/"
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 100m
            memory: 128Mi
        volumeMounts:
        - name: swagger-config
          mountPath: /usr/share/nginx/html/swagger.yaml
          subPath: swagger.yaml
      volumes:
      - name: swagger-config
        configMap:
          name: swagger-config
---
apiVersion: v1
kind: Service
metadata:
  name: swagger-ui-service
  namespace: default
  labels:
    app: swagger-ui
spec:
  type: LoadBalancer
  loadBalancerIP: "172.21.169.79"
  selector:
    app: swagger-ui
  ports:
  - name: http
    port: 8080
    targetPort: 8080
  annotations:
    metallb.universe.tf/loadBalancerIPs: "172.21.169.79"
EOF

kubectl apply -f swagger-ui.yaml
```

### 9.3 é…ç½® Nginx Ingress è¦å‰‡
```bash
cat > swagger-ingress.yaml << 'EOF'
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: swagger-ui-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: swagger.172.21.169.73.nip.io
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: swagger-ui-service
            port:
              number: 8080
EOF

kubectl apply -f swagger-ingress.yaml
```

### 9.4 é©—è­‰ Swagger UI å®‰è£
```bash
# æª¢æŸ¥ Swagger UI Pod
kubectl get pods -n default | grep swagger

# æª¢æŸ¥æœå‹™
kubectl get svc -n default swagger-ui-service

# æ¸¬è©¦è¨ªå•
curl -I http://172.21.169.79:8080
```

---

# æ›´æ–°å¾Œçš„æ•´é«”é©—è­‰

## 1. æª¢æŸ¥æ‰€æœ‰æœå‹™ç‹€æ…‹
```bash
# æª¢æŸ¥æ‰€æœ‰å‘½åç©ºé–“çš„ Pods
kubectl get pods -A | grep -E "(nginx|istio|prometheus|grafana|jaeger|kiali|elasticsearch|kibana|dashboard|swagger)"

# æª¢æŸ¥æ‰€æœ‰ LoadBalancer Services
kubectl get svc -A | grep LoadBalancer

# æª¢æŸ¥ MetalLB IP åˆ†é…
kubectl get svc -A -o wide | grep -E "(172.21.169.7[1-9]|172.21.169.8[0-2])"
```

## 2. å®Œæ•´é€£é€šæ€§æ¸¬è©¦
```bash
echo "=== é€£é€šæ€§æ¸¬è©¦ ==="
echo "Testing Nginx Ingress (172.21.169.73)..."
curl -I http://172.21.169.73

echo "Testing Istio Ingress (172.21.169.72)..."
curl -I http://172.21.169.72

echo "Testing Prometheus (172.21.169.75:9090)..."
curl -I http://172.21.169.75:9090

echo "Testing Grafana (172.21.169.74)..."
curl -I http://172.21.169.74

echo "Testing Jaeger UI (172.21.169.82:16686)..."
curl -I http://172.21.169.82:16686

echo "Testing Kiali (172.21.169.77:20001)..."
curl -I http://172.21.169.77:20001

echo "Testing Kibana (172.21.169.71:5601)..."
curl -I http://172.21.169.71:5601

echo "Testing K8s Dashboard (172.21.169.81:8443)..."
curl -k -I https://172.21.169.81:8443

echo "Testing Swagger UI (172.21.169.79:8080)..."
curl -I http://172.21.169.79:8080
```

## 3. åŠŸèƒ½æ•´åˆæ¸¬è©¦
```bash
# æ¸¬è©¦ Jaeger è¿½è¹¤åŠŸèƒ½
kubectl exec -n istio-system $(kubectl get pod -n istio-system -l app=istiod -o jsonpath='{.items[0].metadata.name}') -- \
  curl -s http://jaeger-production-collector.observability.svc.cluster.local:14268/api/traces

# æ¸¬è©¦ Kiali æœå‹™ç¶²æ ¼è¦–è¦ºåŒ–
curl -s http://172.21.169.77:20001/api/namespaces | jq '.[] | .name'

# æ¸¬è©¦ Elasticsearch ç´¢å¼•
kubectl exec -n logging elasticsearch-master-0 -- curl -s http://localhost:9200/_cat/indices

# ç²å– Dashboard Token
echo "Kubernetes Dashboard Token:"
kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath={".data.token"} | base64 -d
echo
```

---

# æ›´æ–°å¾Œçš„æ¸…ç†æŒ‡ä»¤

å¦‚éœ€ç§»é™¤æ‰€æœ‰æœå‹™ï¼š
```bash
# åˆªé™¤æ‡‰ç”¨æœå‹™
kubectl delete -f swagger-ui.yaml
kubectl delete -f dashboard-admin.yaml
kubectl delete -f test-microservice.yaml

# å¸è¼‰ Helm releases
helm uninstall swagger-ui -n default || true
helm uninstall filebeat -n logging
helm uninstall kibana -n logging  
helm uninstall elasticsearch -n logging
helm uninstall kiali-operator -n kiali-operator
helm uninstall grafana -n monitoring
helm uninstall prometheus -n monitoring
helm uninstall nginx-ingress -n ingress-nginx

# åˆªé™¤ Jaeger å’Œ Kiali å¯¦ä¾‹
kubectl delete -f jaeger-instance.yaml
kubectl delete -f kiali-instance.yaml

# åˆªé™¤ Kubernetes Dashboard
kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

# åˆªé™¤ Istio
istioctl uninstall --purge -y

# åˆªé™¤ Jaeger Operator
kubectl delete -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.57.0/jaeger-operator.yaml -n observability

# åˆªé™¤å‘½åç©ºé–“
kubectl delete namespace monitoring logging observability istio-system ingress-nginx kubernetes-dashboard kiali-operator

# æ¸…ç† MetalLB é…ç½®
kubectl delete -f metallb-*-config.yaml

# æ¸…ç†é…ç½®æ–‡ä»¶
rm -f *.yaml
```

---

# å¯¦éš›éƒ¨ç½²ç¶“é©—ç¸½çµ

## å·²çŸ¥å•é¡Œå’Œè§£æ±ºæ–¹æ¡ˆ

### Prometheus LoadBalancer é…ç½®å•é¡Œ
- **å•é¡Œ**ï¼šåŽŸå§‹é…ç½®ä¸­çš„ `server` éƒ¨åˆ†å° kube-prometheus-stack ç„¡æ•ˆ
- **ç¾è±¡**ï¼šæœå‹™å‰µå»ºç‚º ClusterIP è€Œéž LoadBalancerï¼ŒIP ç„¡æ³•è¨ªå•
- **è§£æ±ºæ–¹æ¡ˆ**ï¼šæ‰‹å‹•å‰µå»º LoadBalancer æœå‹™æˆ–ä¿®æ­£ Helm values
- **ç‹€æ…‹**ï¼šâœ… å·²è§£æ±ºä¸¦é©—è­‰

### é…ç½®çµæ§‹å·®ç•°
- **éŒ¯èª¤é…ç½®**ï¼š`server.service` (ä¾†è‡ªæ¨™æº– Prometheus chart)
- **æ­£ç¢ºé…ç½®**ï¼š`prometheus.service` (é©ç”¨æ–¼ kube-prometheus-stack)
- **æ•™è¨“**ï¼šä¸åŒ Helm Chart æœ‰ä¸åŒé…ç½®çµæ§‹ï¼Œéœ€æŸ¥é–±å®˜æ–¹æ–‡æª”

---

# æœ€çµ‚éƒ¨ç½²ç‹€æ…‹

å®Œæˆä»¥ä¸Šæ­¥é©Ÿä¸¦æ‡‰ç”¨ä¿®å¾©å¾Œï¼Œä½ å°‡æ“æœ‰ï¼š

âœ… **Nginx Ingress Controller** (172.21.169.73) - HTTP/HTTPS è·¯ç”±
âœ… **Istio Ingress Gateway** (172.21.169.72) - æœå‹™ç¶²æ ¼å…¥å£  
âœ… **Prometheus** (172.21.169.75:9090) - ç›£æŽ§æ•¸æ“šæ”¶é›† **[å·²ä¿®å¾© LoadBalancer å•é¡Œ]**
âœ… **Grafana** (172.21.169.74) - ç›£æŽ§è¦–è¦ºåŒ–
âœ… **Jaeger UI** (172.21.169.82:16686) - åˆ†æ•£å¼è¿½è¹¤
âœ… **Kiali** (172.21.169.77:20001) - æœå‹™ç¶²æ ¼è¦–è¦ºåŒ–
âœ… **Kibana** (172.21.169.71:5601) - æ—¥èªŒæœå°‹å’Œè¦–è¦ºåŒ–
âœ… **Kubernetes Dashboard** (172.21.169.81:8443) - å¢é›†ç®¡ç†ç•Œé¢
âœ… **Swagger UI** (172.21.169.79:8080) - API æ–‡ä»¶å’Œæ¸¬è©¦

æ‰€æœ‰æœå‹™éƒ½é…ç½®äº†å›ºå®š IP åœ°å€ï¼Œä¸¦ä¸”æ•´åˆäº†å®Œæ•´çš„ç›£æŽ§ã€æ—¥èªŒã€è¿½è¹¤å’Œç®¡ç†åŠŸèƒ½ï¼Œæ§‹æˆäº†ä¸€å€‹å®Œæ•´çš„å¾®æœå‹™å¯è§€æ¸¬æ€§å¹³å°ã€‚

**å¯¦éš›éƒ¨ç½²é©—è­‰**ï¼šPrometheus å·²æˆåŠŸé€šéŽæ‰‹å‹• LoadBalancer æœå‹™å¯¦ç¾ 172.21.169.75:9090 è¨ªå•ï¼Œæ‰€æœ‰ç›£æŽ§ç›®æ¨™å¥åº·ç‹€æ…‹è‰¯å¥½ã€‚